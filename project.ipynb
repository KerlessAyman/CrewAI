{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb4a73f",
   "metadata": {},
   "source": [
    "#  Web Search Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755db92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: https://wuzzuf.net/search/jobs/?q=Machine Learning&a=hpb&l=Egypt&start=0\n",
      "Error parsing job card: 'href'\n",
      "Scraping page 2: https://wuzzuf.net/search/jobs/?q=Machine Learning&a=hpb&l=Egypt&start=10\n",
      "Total jobs scraped: 14\n",
      "{'title': 'Data Science Instructor', 'company': 'EpsilonAI -', 'location': 'Nasr City, Cairo, Egypt', 'link': 'https://wuzzuf.net/jobs/careers/EpsilonAI-Egypt-65111'}\n",
      "{'title': 'Senior AI Engineer', 'company': 'Lumin -', 'location': 'Sheikh Zayed, Giza, Egypt', 'link': 'https://wuzzuf.net/jobs/careers/Excel-Systems-LLC-Egypt-13729'}\n",
      "{'title': 'Research Engineer', 'company': 'Enoshsciense Research Center -', 'location': 'New Cairo, Cairo, Egypt', 'link': 'https://wuzzuf.net/jobs/careers/Enoshsciense Research Center-Egypt-127306'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_wuzzuf(query=\"AI\", location=\"Egypt\", max_pages=3):\n",
    "    jobs = []\n",
    "    base_url = f\"https://wuzzuf.net/search/jobs/?q={query}&a=hpb&l={location}\"\n",
    "\n",
    "    for page in range(max_pages):\n",
    "        start = page * 10\n",
    "        url = base_url + f\"&start={start}\"\n",
    "        print(f\"Scraping page {page+1}: {url}\")\n",
    "        resp = requests.get(url)\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page+1}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        job_cards = soup.find_all('div', class_='css-1gatmva e1v1l3u10') \n",
    "\n",
    "        for card in job_cards:\n",
    "            try:\n",
    "                title = card.find('h2').text.strip()\n",
    "                company = card.find('a', class_='css-17s97q8').text.strip()\n",
    "                loc = card.find('span', class_='css-5wys0k').text.strip()\n",
    "                link = card.find('a')['href']\n",
    "\n",
    "               \n",
    "                if link.startswith('http'):\n",
    "                    full_link = link\n",
    "                else:\n",
    "                    full_link = \"https://wuzzuf.net\" + link\n",
    "\n",
    "                jobs.append({\n",
    "                    'title': title,\n",
    "                    'company': company,\n",
    "                    'location': loc,\n",
    "                    'link': full_link\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing job card: {e}\")\n",
    "                continue\n",
    "\n",
    "        time.sleep(1)  \n",
    "\n",
    "    return jobs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = scrape_wuzzuf(query=\"Machine Learning\", location=\"Egypt\", max_pages=2)\n",
    "    print(f\"Total jobs scraped: {len(data)}\")\n",
    "    for job in data[:3]:  # عرض أول 3 وظائف كمثال\n",
    "        print(job)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b144b2",
   "metadata": {},
   "source": [
    "# Data Extraction Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61619271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_description(job_url):\n",
    "    try:\n",
    "        resp = requests.get(job_url)\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"Failed to fetch job description: {job_url}\")\n",
    "            return \"\"\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "       \n",
    "        desc_div = soup.find('div', class_='css-1m4cuuf') \n",
    "\n",
    "        if desc_div:\n",
    "            description = desc_div.get_text(separator=' ').strip()\n",
    "            return description\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching job description: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31810075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: https://wuzzuf.net/search/jobs/?q=Machine Learning&a=hpb&l=Egypt&start=0\n",
      "Error parsing job card: 'href'\n",
      "Data Science Instructor\n",
      "\n",
      "--------------------------------------------------\n",
      "Senior AI Engineer\n",
      "\n",
      "--------------------------------------------------\n",
      "Research Engineer\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def scrape_wuzzuf_with_desc(query=\"AI\", location=\"Egypt\", max_pages=2):\n",
    "    jobs = []\n",
    "    base_url = f\"https://wuzzuf.net/search/jobs/?q={query}&a=hpb&l={location}\"\n",
    "\n",
    "    for page in range(max_pages):\n",
    "        start = page * 10\n",
    "        url = base_url + f\"&start={start}\"\n",
    "        print(f\"Scraping page {page+1}: {url}\")\n",
    "        resp = requests.get(url)\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page+1}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        job_cards = soup.find_all('div', class_='css-1gatmva e1v1l3u10')\n",
    "\n",
    "        for card in job_cards:\n",
    "            try:\n",
    "                title = card.find('h2').text.strip()\n",
    "                company = card.find('a', class_='css-17s97q8').text.strip()\n",
    "                loc = card.find('span', class_='css-5wys0k').text.strip()\n",
    "                link = card.find('a')['href']\n",
    "\n",
    "                if link.startswith('http'):\n",
    "                    full_link = link\n",
    "                else:\n",
    "                    full_link = \"https://wuzzuf.net\" + link\n",
    "\n",
    "                description = get_job_description(full_link)\n",
    "\n",
    "                jobs.append({\n",
    "                    'title': title,\n",
    "                    'company': company,\n",
    "                    'location': loc,\n",
    "                    'link': full_link,\n",
    "                    'description': description\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing job card: {e}\")\n",
    "                continue\n",
    "\n",
    "        time.sleep(1)\n",
    "    return jobs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    jobs_data = scrape_wuzzuf_with_desc(query=\"Machine Learning\", location=\"Egypt\", max_pages=1)\n",
    "    for job in jobs_data[:3]:\n",
    "        print(job['title'])\n",
    "        print(job['description'][:300])  # عرض أول 300 حرف من الوصف\n",
    "        print('-'*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bedd0e",
   "metadata": {},
   "source": [
    "# Trend Analysis Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f72832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills_from_description(description, skill_keywords):\n",
    "    found_skills = []\n",
    "    desc_lower = description.lower()\n",
    "    for skill in skill_keywords:\n",
    "        if skill.lower() in desc_lower:\n",
    "            found_skills.append(skill)\n",
    "    return list(set(found_skills)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aff791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_jobs(jobs, skill_keywords):\n",
    "    titles = [job['title'] for job in jobs]\n",
    "    locations = [job['location'] for job in jobs]\n",
    "\n",
    "   \n",
    "    all_skills = []\n",
    "    for job in jobs:\n",
    "        skills = extract_skills_from_description(job.get('description', ''), skill_keywords)\n",
    "        all_skills.extend(skills)\n",
    "\n",
    "\n",
    "    title_counts = Counter(titles)\n",
    "    skill_counts = Counter(all_skills)\n",
    "    location_counts = Counter(locations)\n",
    "\n",
    "    return title_counts.most_common(), skill_counts.most_common(), location_counts.most_common()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "914f99c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: https://wuzzuf.net/search/jobs/?q=Machine Learning&a=hpb&l=Egypt&start=0\n",
      "Error parsing job card: 'href'\n",
      "Scraping page 2: https://wuzzuf.net/search/jobs/?q=Machine Learning&a=hpb&l=Egypt&start=10\n",
      "Top Job Titles:\n",
      "Data Science Instructor: 1\n",
      "Senior AI Engineer: 1\n",
      "Research Engineer: 1\n",
      "Senior AI Vision Engineer – Computer Vision & Edge Deployment: 1\n",
      "Artificial Intelligence Engineer (NLP): 1\n",
      "AI Trainer.: 1\n",
      "Cloud Infrastructure Engineer: 1\n",
      "Fresh Electrical PLC & Planning Maintenace Engineer: 1\n",
      "AI Engineer: 1\n",
      "AI Solutions Architect: 1\n",
      "\n",
      "Top Skills:\n",
      "\n",
      "Job Locations Distribution:\n",
      "New Cairo, Cairo, Egypt: 2\n",
      "Heliopolis, Cairo, Egypt: 2\n",
      "Nasr City, Cairo, Egypt: 1\n",
      "Sheikh Zayed, Giza, Egypt: 1\n",
      "6th of October, Giza, Egypt: 1\n",
      "Ain Sokhna, Suez, Egypt: 1\n",
      "Maadi, Cairo, Egypt: 1\n",
      "Cairo, Egypt: 1\n",
      "Giza, Egypt: 1\n",
      "Makkah, Saudi Arabia: 1\n",
      "Dubai, United Arab Emirates: 1\n",
      "Riyadh, Saudi Arabia: 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    jobs_data = scrape_wuzzuf_with_desc(query=\"Machine Learning\", location=\"Egypt\", max_pages=2)\n",
    "\n",
    "    skill_keywords = [\"Python\", \"TensorFlow\", \"PyTorch\", \"Deep Learning\", \"NLP\", \"SQL\", \"Docker\", \"Keras\", \"Scikit-learn\"]\n",
    "\n",
    "    top_titles, top_skills, top_locations = analyze_jobs(jobs_data, skill_keywords)\n",
    "\n",
    "    print(\"Top Job Titles:\")\n",
    "    for title, count in top_titles[:10]:\n",
    "        print(f\"{title}: {count}\")\n",
    "\n",
    "    print(\"\\nTop Skills:\")\n",
    "    for skill, count in top_skills[:10]:\n",
    "        print(f\"{skill}: {count}\")\n",
    "\n",
    "    print(\"\\nJob Locations Distribution:\")\n",
    "    for loc, count in top_locations:\n",
    "        print(f\"{loc}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b55d2",
   "metadata": {},
   "source": [
    "#  Report Writer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0841245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_markdown_report(top_roles, top_skills, location_distribution, trends_summary):\n",
    "    md = \"# Top AI/ML Jobs in MENA – May 2025\\n\\n\"\n",
    "\n",
    "    md += \"## Top 10 AI/ML Roles\\n\"\n",
    "    for i, (role, count) in enumerate(top_roles[:10], 1):\n",
    "        md += f\"{i}. {role} ({count} postings)\\n\"\n",
    "\n",
    "    md += \"\\n## Key Skills Required\\n\"\n",
    "    md += \", \".join([skill for skill, _ in top_skills[:15]]) + \"\\n\"\n",
    "\n",
    "    md += \"\\n## Country-wise Job Distribution\\n\"\n",
    "    for country, count in location_distribution:\n",
    "        md += f\"- {country}: {count} jobs\\n\"\n",
    "\n",
    "    md += \"\\n## Trends & Observations\\n\"\n",
    "    md += trends_summary + \"\\n\"\n",
    "\n",
    "    return md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e87a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trends_summary(top_roles, top_skills):\n",
    "    summary = f\"The role '{top_roles[0][0]}' is currently the most demanded position, with {top_roles[0][1]} job postings. \"\n",
    "    summary += f\"Key skills in demand include {', '.join([skill for skill, _ in top_skills[:5]])}. \"\n",
    "    summary += \"The market shows strong growth in AI/ML jobs across the MENA region, especially in Egypt, UAE, and Saudi Arabia.\"\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93724281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Top AI/ML Jobs in MENA – May 2025\n",
      "\n",
      "## Top 10 AI/ML Roles\n",
      "1. Machine Learning Engineer (120 postings)\n",
      "2. Data Scientist (80 postings)\n",
      "3. AI Researcher (40 postings)\n",
      "\n",
      "## Key Skills Required\n",
      "Python, TensorFlow, Deep Learning, NLP, SQL\n",
      "\n",
      "## Country-wise Job Distribution\n",
      "- Egypt: 150 jobs\n",
      "- UAE: 70 jobs\n",
      "- Saudi Arabia: 60 jobs\n",
      "\n",
      "## Trends & Observations\n",
      "The role 'Machine Learning Engineer' is currently the most demanded position, with 120 job postings. Key skills in demand include Python, TensorFlow, Deep Learning, NLP, SQL. The market shows strong growth in AI/ML jobs across the MENA region, especially in Egypt, UAE, and Saudi Arabia.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    top_roles = [('Machine Learning Engineer', 120), ('Data Scientist', 80), ('AI Researcher', 40)]\n",
    "    top_skills = [('Python', 90), ('TensorFlow', 75), ('Deep Learning', 60), ('NLP', 55), ('SQL', 50)]\n",
    "    location_distribution = [('Egypt', 150), ('UAE', 70), ('Saudi Arabia', 60)]\n",
    "\n",
    "    trends_summary = create_trends_summary(top_roles, top_skills)\n",
    "    report_md = generate_markdown_report(top_roles, top_skills, location_distribution, trends_summary)\n",
    "\n",
    "    print(report_md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7d2e7",
   "metadata": {},
   "source": [
    "# CLI Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1741b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_report(report_md, max_lines=20):\n",
    "    lines = report_md.split('\\n')\n",
    "    preview_lines = lines[:max_lines]\n",
    "    print(\"\\n\".join(preview_lines))\n",
    "    if len(lines) > max_lines:\n",
    "        print(\"\\n... (Report truncated for preview) ...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0bc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Top AI/ML Jobs in MENA – May 2025\n",
      "\n",
      "## Top 10 AI/ML Roles\n",
      "1. Machine Learning Engineer (120 postings)\n",
      "2. Data Scientist (80 postings)\n",
      "3. AI Researcher (40 postings)\n",
      "\n",
      "## Key Skills Required\n",
      "Python, TensorFlow, Deep Learning, NLP, SQL\n",
      "\n",
      "## Country-wise Job Distribution\n",
      "- Egypt: 150 jobs\n",
      "- UAE: 70 jobs\n",
      "- Saudi Arabia: 60 jobs\n",
      "\n",
      "## Trends & Observations\n",
      "The role 'Machine Learning Engineer' is currently the most demanded position, with 120 job postings. Key skills in demand include Python, TensorFlow, Deep Learning, NLP, SQL. The market shows strong growth in AI/ML jobs across the MENA region, especially in Egypt, UAE, and Saudi Arabia.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    preview_report(report_md, max_lines=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
